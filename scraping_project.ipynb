{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4462b90a-5e65-48ef-9def-75fe05a80a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlparse, urljoin\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15416a2d-f541-4e9a-95c2-32a887973169",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_urls = set() # links of the website\n",
    "external_urls = set() # links of other website\n",
    "bad_urls = set() # not properly formatted links\n",
    "error_urls = set() # links with connection errors\n",
    "total_urls_visited = 0 #  counter for links visited by crawler\n",
    "max_urls=3 # maximum number of links to be visited by crawler\n",
    "ConnectErrs  = (ConnectionError, OSError, ConnectionResetError) # list of errors to be catched\n",
    "scrapped_home_data = pd.DataFrame() #dataframe to hold all scrapped data for home products\n",
    "scrapped_business_data = pd.DataFrame() #dataframe to hold all scrapped data for business products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc287fa-e918-42fe-9974-10f9adae119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    if url.startswith('mailto') or url.startswith('tel') or url.startswith('/brand'):\n",
    "        bad_urls .add(url)\n",
    "        return False\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ea482d-d51e-47e8-af82-0482b38227a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_website_links(url):\n",
    "    \"\"\"\n",
    "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
    "    \"\"\"\n",
    "    # all URLs of `url`\n",
    "    urls = set()\n",
    "    # domain name of the URL without the protocol\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "          # join the URL if it's relative (not absolute link)\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        # remove URL GET parameters, URL fragments, etc.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            # not a valid URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # already in the set\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # external link\n",
    "            if href not in external_urls:\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17db34d-c107-4a59-a074-05585d6e5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    \"\"\"\n",
    "    Crawls a web page and extracts all links.\n",
    "    You'll find all links in `external_urls` and `internal_urls` global set variables.\n",
    "    links with connections issues are in 'error_urls'\n",
    "    \"\"\"\n",
    "    global total_urls_visited\n",
    "    global max_urls\n",
    "    global cpbar\n",
    "    total_urls_visited += 1\n",
    "    cpbar.update(1)\n",
    "    links = get_all_website_links(url)\n",
    "    for link in links:\n",
    "        if total_urls_visited >= max_urls:\n",
    "            cpbar.close()\n",
    "            break\n",
    "        try:\n",
    "            crawl(link)\n",
    "        except ConnectErrs as e:\n",
    "            error_urls.add(url)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386df46f-aa1a-48d9-ad37-97aa814d178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_home(link):\n",
    "    global scrapped_home_data\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    html = list(soup.children)[1]\n",
    "    body = list(html.children)[1]\n",
    "    if len(str(list(body.children)[-1])) > 48: # check if data available\n",
    "        item_dict = eval(str(list(body.children)[-1])[35:-9].replace('\\n','').replace('@','').replace('[','').replace(']',''))\n",
    "        df = pd.json_normalize(item_dict, record_prefix='Prefix.') # to be used with single record\n",
    "        image_element = soup.find_all(\"img\")[2]['src'] # scrap the image link\n",
    "        df['image'] = image_element # adding the image link to results\n",
    "        page_text = soup.get_text('/n') # add seperator so you can split later\n",
    "        relevant_beginning = page_text.find('Main') + 15\n",
    "        relevant_ending = page_text.find('addGeneral Terms')\n",
    "        product_and_related = page_text[relevant_beginning:relevant_ending]\n",
    "        details_beginning = product_and_related.find('Details') + 7\n",
    "        details_ending  = product_and_related.find('option')\n",
    "        product_details = product_and_related[details_beginning:details_ending]\n",
    "        related = product_and_related[details_ending:]\n",
    "        product_info = product_details.split('/n')[1:-1]\n",
    "        keys = product_info[::2]\n",
    "        values = product_info[1::2]\n",
    "        for key, value in zip(keys,values): # add the additional data to the dataframe\n",
    "            df[key] = value\n",
    "\n",
    "        scrapped_home_data = scrapped_home_data.append(df, ignore_index=True) # update the global daframe\n",
    "        hspbar.update(1)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "        image_element = soup.find_all(\"img\")[2]['src'] # scrap the image link\n",
    "        df['image'] = image_element # adding the image link to results\n",
    "        page_text = soup.get_text('/n') # add seperator so you can split later\n",
    "        relevant_beginning = page_text.find('Main') + 15\n",
    "        relevant_ending = page_text.find('addGeneral Terms')\n",
    "        product_and_related = page_text[relevant_beginning:relevant_ending]\n",
    "        details_beginning = product_and_related.find('Details') + 7\n",
    "        details_ending  = product_and_related.find('Select')\n",
    "        product_details = product_and_related[details_beginning:details_ending]\n",
    "        related = product_and_related[details_ending:]\n",
    "        product_info = product_details.split('/n')[1:-1]\n",
    "        keys = product_info[::2]\n",
    "        values = product_info[1::2]\n",
    "        for key, value in zip(keys,values): # add the additional data to the dataframe\n",
    "            df[key] = value\n",
    "\n",
    "        scrapped_home_data = scrapped_home_data.append(df, ignore_index=True) # update the global daframe\n",
    "        hspbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82d57fd-4c5e-4dad-b9c5-ff1b61736e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_business(link):\n",
    "    global scrapped_business_data\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    html = list(soup.children)[1]\n",
    "    body = list(html.children)[1]\n",
    "    if len(str(list(body.children)[-1])) > 48: # check if data available\n",
    "        item_dict = eval(str(list(body.children)[-1])[35:-9].replace('\\n','').replace('@','').replace('[','').replace(']',''))\n",
    "        df = pd.json_normalize(item_dict, record_prefix='Prefix.') # to be used with single record\n",
    "        image_element = soup.find_all(\"img\")[2]['src'] # scrap the image link\n",
    "        df['image'] = image_element # adding the image link to results\n",
    "        page_text = soup.get_text('/n') # add seperator so you can split later\n",
    "        relevant_beginning = page_text.find('Main') + 15\n",
    "        relevant_ending = page_text.find('addGeneral Terms')\n",
    "        product_and_related = page_text[relevant_beginning:relevant_ending]\n",
    "        details_beginning = product_and_related.find('Details') + 7\n",
    "        details_ending  = product_and_related.find('option')\n",
    "        product_details = product_and_related[details_beginning:details_ending]\n",
    "        related = product_and_related[details_ending:]\n",
    "        product_info = product_details.split('/n')[1:-1]\n",
    "        keys = product_info[::2]\n",
    "        values = product_info[1::2]\n",
    "        for key, value in zip(keys,values): # add the additional data to the dataframe\n",
    "            df[key] = value\n",
    "\n",
    "        scrapped_business_data = scrapped_business_data.append(df, ignore_index=True) # update the global daframe\n",
    "        bspbar.update(1)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "        image_element = soup.find_all(\"img\")[2]['src'] # scrap the image link\n",
    "        df['image'] = image_element # adding the image link to results\n",
    "        page_text = soup.get_text('/n') # add seperator so you can split later\n",
    "        relevant_beginning = page_text.find('Main') + 15\n",
    "        relevant_ending = page_text.find('addGeneral Terms')\n",
    "        product_and_related = page_text[relevant_beginning:relevant_ending]\n",
    "        details_beginning = product_and_related.find('Details') + 7\n",
    "        details_ending  = product_and_related.find('option')\n",
    "        product_details = product_and_related[details_beginning:details_ending]\n",
    "        related = product_and_related[details_ending:]\n",
    "        product_info = product_details.split('/n')[1:-1]\n",
    "        keys = product_info[::2]\n",
    "        values = product_info[1::2]\n",
    "        for key, value in zip(keys,values): # add the additional data to the dataframe\n",
    "            df[key] = value\n",
    "\n",
    "        scrapped_business_data = scrapped_business_data.append(df, ignore_index=True) # update the global daframe\n",
    "        bspbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b98ce03-ac5e-4077-8b95-1cf1eaaa607c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of Pages to Crawl: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c892cb95eb46d08b687681c2ee747f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Crawling Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] External Links Found: 5\n",
      "[+] Internal Links Found: 70\n",
      "[+] Total: 75\n",
      "[+] Valid Links to Scrap: 57\n",
      "[+] Invalid Links: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1e7fdfa2564da4a92d18b5b76eb18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping Home Products Progress:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc869547d62d4d03bd3d3b7b508bfef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping Business Products Progress:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"[+] Number of Pages to Crawl:\", max_urls)\n",
    "    cpbar = tqdm(total=max_urls, desc=\"Crawling Progress\")\n",
    "    crawl('https://jumla.io/category/main')\n",
    "    print(\"[+] External Links Found:\", len(external_urls))\n",
    "    print(\"[+] Internal Links Found:\", len(internal_urls))\n",
    "    print(\"[+] Total:\", len(external_urls) + len(internal_urls))\n",
    "    internal_urls_list = list(internal_urls)\n",
    "    products_links = [link for link in internal_urls_list if '/item/' in link] # desired links\n",
    "    not_products_links = [link for link in internal_urls_list if '/item/' not in link] # discarded links\n",
    "    home_products_links = [link for link in products_links if '/for-home' in link]\n",
    "    business_products_links = [link for link in products_links if '/for-business' in link] \n",
    "    print(\"[+] Valid Links to Scrap:\", len(products_links))\n",
    "    print(\"[+] Invalid Links:\", len(not_products_links) + len(external_urls))\n",
    "    hspbar = tqdm(total=len(home_products_links), desc=\"Scraping Home Products Progress\")\n",
    "    for hlink in home_products_links:\n",
    "        scrap_home(hlink)\n",
    "    hspbar.close()\n",
    "    bspbar = tqdm(total=len(business_products_links), desc=\"Scraping Business Products Progress\")\n",
    "    for blink in business_products_links:\n",
    "        scrap_business(blink)\n",
    "    bspbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2e494-3848-4fae-89b4-13972be83d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
